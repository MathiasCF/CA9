Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@book{Sutton2020,
author = {Sutton, Richard S. and Barto, Andrew G.},
edition = {2. Edition},
file = {:C\:/Users/Mathias/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sutton, Barto - 2020 - Reinforcement Learning - An introduction.pdf:pdf},
isbn = {9780262039246},
pages = {1--548},
title = {{Reinforcement Learning - An introduction}},
url = {https://drive.google.com/file/d/1hQWghwBe-LLVZYh8vtUqe5HeG-9PUSkK/view},
year = {2020}
}
@article{Val2020,
abstract = {Optimal control for Water Distribution Networks (WDN) is subject to complex system models. Typically, detailed models are not available or the implementation is too expensive for small utilities. Reinforcement Learning (RL) methods are well known techniques for model-free control. This paper proposes a model-free controller for WDNs based on RL methods and presents experimental evidence of the practicality of the design.},
author = {Val, Jorge and Wisniewski, Rafal and Kallesoe, Carsten S.},
doi = {10.1016/j.ifacol.2020.12.075},
file = {:C\:/Users/Mathias/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Val, Wisniewski, Kallesoe - 2020 - Optimal control for water distribution networks with unknown dynamics(2).pdf:pdf},
issn = {24058963},
journal = {IFAC-PapersOnLine},
keywords = {Level Control,Reinforcement Learning,Water Distribution Networks},
month = {jan},
number = {2},
pages = {6577--6582},
publisher = {Elsevier},
title = {{Optimal control for water distribution networks with unknown dynamics}},
volume = {53},
year = {2020}
}
@article{DaMottaSallesBarreto2008,
abstract = {This work presents the restricted gradient-descent (RGD) algorithm, a training method for local radial-basis function networks specifically developed to be used in the context of reinforcement learning. The RGD algorithm can be seen as a way to extract relevant features from the state space to feed a linear model computing an approximation of the value function. Its basic idea is to restrict the way the standard gradient-descent algorithm changes the hidden units of the approximator, which results in conservative modifications that make the learning process less prone to divergence. The algorithm is also able to configure the topology of the network, an important characteristic in the context of reinforcement learning, where the changing policy may result in different requirements on the approximator structure. Computational experiments are presented showing that the RGD algorithm consistently generates better value-function approximations than the standard gradient-descent method, and that the latter is more susceptible to divergence. In the pole-balancing and Acrobot tasks, RGD combined with SARSA presents competitive results with other methods found in the literature, including evolutionary and recent reinforcement-learning algorithms. {\textcopyright} 2007 Elsevier B.V. All rights reserved.},
author = {{da Motta Salles Barreto}, Andr{\'{e}} and Anderson, Charles W.},
doi = {10.1016/j.artint.2007.08.001},
file = {:C\:/Users/Mathias/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/da Motta Salles Barreto, Anderson - 2008 - Restricted gradient-descent algorithm for value-function approximation in reinforcement learn.pdf:pdf},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Neuro-dynamic programming,Radial-basis-function networks,Reinforcement learning,Value-function approximation},
number = {4-5},
pages = {454--482},
title = {{Restricted gradient-descent algorithm for value-function approximation in reinforcement learning}},
volume = {172},
year = {2008}
}
@article{Jensen,
author = {Jensen, Tom N{\o}rgaard},
file = {:C\:/Users/Mathias/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jensen - Unknown - Modelling of open hydraulic networks.pdf:pdf},
pages = {1--8},
title = {{Modelling of open hydraulic networks}}
}
@article{MathiasJeppe730,
abstract = {High quality control of Water Distribution Net- works (WDN) is critical to reliable supply of drinking water to consumers, but is subject to complex nonlinear dynamics and losses induced by actuator characteristics, unmeasured disturbances, and long-distance wireless communication. This paper achieves constant elevated water reservoir (EWR) pressure via an outer velocity-form Linear Quadratic Regulator (VF-LQR) and an inner PI pump control loop. Disturbance rejection in the outer loop and leakage detection are facilitated by a Kalman filter (KF) based on a harmonic disturbance model. Comprehensive experimental validation of the proposed control paradigm is presented, and an analytical proof of marginal system stability for arbitrarily large packet loss under the assumption of a Try- Once-and-Discard (TOD) protocol is given. Keywords: Water distribution, LQR, networked control.},
author = {Andersen, Laurits Hastrup and Frederiksen, Mathias Clement and Jensen, Christian M{\o}ller and Jensen, Jeppe N{\o}rgaard and Kristiansen, Rasmus L{\o}vschall and Laustsen, Kasper and Therkildsen, Martin H{\o}jlund},
file = {:C\:/Users/Mathias/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Andersen et al. - 2021 - Modelling and Networked Control of Water Distribution Networks.pdf:pdf},
journal = {AAU conf},
keywords = {lqr,networked control,water distribution},
mendeley-tags = {lqr,networked control,water distribution},
pages = {6},
title = {{Modelling and Networked Control of Water Distribution Networks}},
url = {https://projekter.aau.dk/projekter/da/studentthesis/modelling-and-networked-control-of-water-distribution-networks(298dfc57-8338-4660-8cd6-0f802f6de620).html},
year = {2021}
}
@article{Val2021a,
abstract = {Cost efficient management of Water Distribution Networks with storage units requires of extensive knowledge of the water network. However, the network models are not always available or the calibration costs are too high for most of small water utilities. This paper proposes a model-free control solution based on Q-learning methods that provides a policy for the operation of the network. This supervisory controller must guarantee the water supply despite of the uncertainty of the daily water consumption and reduce the operation cost. The function approximation proposed for the Q-learning controller uses Fourier Basis Functions which provide an accurate approximation of the periodic disturbances. This paper presents results of the control validation in a simulation framework as well as experimental evidence of the advantages and limitations of the proposed design.},
author = {Val, Jorge and Wisniewski, Rafal and Kallesoe, Carsten Skovmose},
doi = {10.23919/ACC50511.2021.9482787},
file = {:C\:/Users/Mathias/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Val, Wisniewski, Kallesoe - 2021 - Reinforcement Learning Control for Water Distribution Networks with Periodic DisturbancesThis work wa.pdf:pdf},
isbn = {9781665441971},
issn = {07431619},
journal = {Proceedings of the American Control Conference},
pages = {1010--1015},
title = {{Reinforcement Learning Control for Water Distribution Networks with Periodic Disturbances**This work was supported by Poul Due Jensens Foundation}},
volume = {2021-May},
year = {2021}
}
@article{Val2021,
abstract = {Reinforcement Learning (RL) is an optimal control method for regulating the behaviour of a dynamical system when the system model is unknown. This feature is a strong advantage for controlling systems, such as Water Distribution Networks, where it is difficult to have a reliable model. When learning an optimal policy with RL, the exploration phase implies high degree of uncertainty in the system operation. Large scale infrastructures such as WDN require a robust operation since they cannot afford fails during the operation. This paper presents a model-free control method which provides safety in the operation while learning an optimal policy. This method introduces a policy supervisor block in the control loop which assesses the safety of the learned policy in real-time. The safety verification consists of evaluating the trajectory on a standard linear model. In this model only the fundamental linear dynamics are represented and the system's dimensions do not require to be expressed with high accuracy. If the predicted trajectory violates the boundaries, the supervisor provides a safe control action. Simulation and experimental results prove the applicability of the proposed method.},
author = {Val, Jorge and Wisniewski, Rafal and Kallesoe, Carsten Skovmose},
doi = {10.1109/CCTA48906.2021.9659138},
file = {:C\:/Users/Mathias/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Val, Wisniewski, Kallesoe - 2021 - Safe Reinforcement Learning Control for Water Distribution Networks.pdf:pdf},
isbn = {9781665436434},
journal = {CCTA 2021 - 5th IEEE Conference on Control Technology and Applications},
pages = {1148--1153},
title = {{Safe Reinforcement Learning Control for Water Distribution Networks}},
year = {2021}
}
@article{WaterResourcesGroup2009,
abstract = {Constraints on a valuable resource should draw new investment and prompt policies to increase productivity of demand and augment supply. However, for water, arguably one of the most constrained and valuable resources we have, this does not seem to be happening. Calls for action multiply and yet an abundance of evidence shows that the situation is getting worse. There is little indication that, left to its own devices, the water sector will come to a sustainable, cost-effective solution to meet the growing water requirements implied by economic and population growth. This study focuses on how, by 2030, competing demands for scarce water resources can be met and sustained. It is sponsored, written, and supported by a group of private sector companies and institutions who are concerned about water scarcity as an increasing business risk, a major economic threat that cannot be ignored, and a global priority that affects human well-being.},
author = {{Water Resources Group}},
file = {::},
journal = {Water},
number = {3},
pages = {1--32},
title = {{Charting Our Water Future}},
url = {http://www.mckinsey.com/App_Media/Reports/Water/Charting_Our_Water_Future_Full_Report_001.pdf%5Cnhttp://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:Charting+Our+Water+Future#1%5Cnhttp://www.mckinsey.com/App_Media/Reports/Water/Charting_Our_Wate},
volume = {June},
year = {2009}
}
@misc{EuroStat,
abstract = {The article presents a handful of findings from the more detailed Statistics Explained articles on electricity prices and natural gas prices.},
author = {EuroStat},
title = {{Electricity and gas prices in the first half of 2022}},
url = {https://ec.europa.eu/eurostat/web/products-eurostat-news/-/ddn-20221031-1}
}
@article{Rathore930,
author = {Rathore, Saruch Satishkumar},
file = {:C\:/Users/Mathias/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rathore - 2019 - Optimal Control in Water Distribution Network.pdf:pdf},
title = {{Optimal Control in Water Distribution Network}},
year = {2019}
}
@article{Busoniu2010,
abstract = {From household appliances to applications in robotics, engineered systems involving complex dynamics can only be as effective as the algorithms that control them. While Dynamic Programming (DP) has provided researchers with a way to optimally solve decision and control problems involving complex dynamic systems, its practical value was limited by algorithms that lacked the capacity to scale up to realistic problems. However, in recent years, dramatic developments in Reinforcement Learning (RL), the model-free counterpart of DP, changed our understanding of what is possible. Those developments led to the creation of reliable methods that can be applied even when a mathematical model of the system is unavailable, allowing researchers to solve challenging control problems in engineering, as well as in a variety of other disciplines, including economics, medicine, and artificial intelligence. Reinforcement Learning and Dynamic Programming Using Function Approximators provides a comprehensive and unparalleled exploration of the field of RL and DP. With a focus on continuous-variable problems, this seminal text details essential developments that have substantially altered the field over the past decade. In its pages, pioneering experts provide a concise introduction to classical RL and DP, followed by an extensive presentation of the state-of-the-art and novel methods in RL and DP with approximation. Combining algorithm development with theoretical guarantees, they elaborate on their work with illustrative examples and insightful comparisons. Three individual chapters are dedicated to representative algorithms from each of the major classes of techniques: value iteration, policy iteration, and policy search. The features and performance of these algorithms are highlighted in extensive experimental studies on a range of control applications. The recent development of applications involving complex systems has led to a surge of interest in RL and DP methods and the subsequent need for a quality resource on the subject. For graduate students and others new to the field, this book offers a thorough introduction to both the basics and emerging methods. And for those researchers and practitioners working in the fields of optimal and adaptive control, machine learning, artificial intelligence, and operations research, this resource offers a combination of practical algorithms, theoretical analysis, and comprehensive examples that they will be able to adapt and apply to their own work. Access the authors' website at www.dcsc.tudelft.nl/rlbook/ for additional material, including computer code used in the studies and information concerning new developments.},
author = {Buşoniu, Lucian and Babu{\v{s}}ka, Robert and {De Schutter}, Bart and Ernst, Damien},
doi = {10.1201/9781439821091},
file = {:C\:/Users/Mathias/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Buşoniu et al. - 2010 - Reinforcement learning and dynamic programming using function approximators.pdf:pdf},
isbn = {9781439821091},
journal = {Reinforcement Learning and Dynamic Programming Using Function Approximators},
pages = {1--271},
title = {{Reinforcement learning and dynamic programming using function approximators}},
year = {2010}
}
@misc{UN_SDG6,
author = {UN},
title = {{UN Sustainable Development Goal 6}},
url = {https://sdgs.un.org/goals/goal6}
}
@article{FAOandUNWater2021,
abstract = {Global baseline for SDG indicator 6.4.2},
author = {{FAO and UN Water}},
file = {::},
journal = {Progress on the level of water stress},
title = {{Progress on the level of water stress. Global status and acceleration needs for SDG Indicator 6.4.2, 2021}},
url = {https://www.unwater.org/app/uploads/2021/08/SDG6_Indicator_Report_642_Progress-on-Level-of-Water-Stress_2021_ENGLISH_pages-1.pdf (accessed 28.08.2022)},
year = {2021}
}
@misc{UN_SDG6162,
author = {UN},
title = {{UN Sustainable Development Goal 6.1, 6.2}},
url = {https://www.un.org/sustainabledevelopment/water-and-sanitation/}
}
@book{Overgaard2019,
author = {Overgaard, Anders},
file = {:C\:/Users/Mathias/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Overgaard - 2019 - Aalborg Universitet Reinforcement Learning for Building Heating via Mixing Loopsserien for Det Tekniske Fakultet for.pdf:pdf},
isbn = {9788772105444},
title = {{Aalborg Universitet Reinforcement Learning for Building Heating via Mixing Loopsserien for Det Tekniske Fakultet for IT og Design, Aalborg Universitet}},
year = {2019}
}
@phdthesis{Rathore1030,
author = {Rathore, Saruch Satishkumar},
file = {:C\:/Users/Mathias/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rathore - 2020 - Nonlinear Optimal Control in Water Distribution Network(2).pdf:pdf},
school = {Aalborg University},
title = {{Nonlinear Optimal Control in Water Distribution Network}},
type = {Master's thesis},
year = {2020}
}
